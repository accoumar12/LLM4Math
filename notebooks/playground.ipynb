{"cells":[{"cell_type":"markdown","metadata":{"id":"uP3m4XXExipo"},"source":["# Play with LLMs"]},{"cell_type":"markdown","metadata":{"id":"_KcM5BCAxips"},"source":["## Imports"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":111317,"status":"ok","timestamp":1699746973151,"user":{"displayName":"nicolas","userId":"12558200110464564435"},"user_tz":-60},"id":"GC5W47s5xipt","outputId":"d4c61351-07d8-4599-d927-dfe3f280ac16"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[31mERROR: Could not find a version that satisfies the requirement huggingface-hub<1.0,>=0.19.3 (from transformers) (from versions: 0.0.1, 0.0.2, 0.0.3rc1, 0.0.3rc2, 0.0.5, 0.0.6, 0.0.7, 0.0.8, 0.0.9, 0.0.10, 0.0.11, 0.0.12, 0.0.13, 0.0.14, 0.0.15, 0.0.16, 0.0.17, 0.0.18, 0.0.19, 0.1.0, 0.1.1, 0.1.2, 0.2.0, 0.2.1, 0.4.0)\u001b[0m\n","\u001b[31mERROR: No matching distribution found for huggingface-hub<1.0,>=0.19.3\u001b[0m\n","\u001b[31mERROR: Package 'accelerate' requires a different Python: 3.6.9 not in '>=3.8.0'\u001b[0m\n"]}],"source":["pip install -U bitsandbytes\n","pip install -U git+https://github.com/huggingface/transformers.git\n","\n","pip install -U optimum\n","pip install -U git+https://github.com/huggingface/accelerate.git\n","\n","#!pip install -q -U langchain\n","#!pip install -q -U ctransformers[cuda]\n","#!pip install sentence-transformers\n"]},{"cell_type":"markdown","metadata":{"id":"Ui9y4YBkxipw"},"source":["## Playground Hugging-Face"]},{"cell_type":"markdown","metadata":{"id":"M4UkFjqmxip2"},"source":["Lien entre les noms de modèle et leur \"adresse\" sur hugging face"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"Jg5vJ_lsxip2"},"outputs":[],"source":["# Hugging-Face model ids\n","models_id = {\n","    ### Mistral-based ###\n","    \"mistral7b_instruct\" : \"mistralai/Mistral-7B-Instruct-v0.1\",\n","    \"mistral7b_orca\" : \"Open-Orca/Mistral-7B-OpenOrca\",\n","    \"zephyr7b\" : \"HuggingFaceH4/zephyr-7b-beta\",\n","    \"vigostral7b\" : \"bofenghuang/vigostral-7b-chat\",\n","\n","#    \"mistral7b_original\" : \"mistralai/Mistral-7B-v0.1\",\n","\n","    ### Llama-based ###\n","    \"llama2-chat7b\" : \"meta-llama/Llama-2-7b-chat-hf\",\n","    \"llama2-chat13b\" : \"meta-llama/Llama-2-13b-chat-hf\",\n","\n","    \"vigogne7b\" : \"bofenghuang/vigogne-2-7b-chat\",\n","    \"vigogne7b_instruct\" : \"bofenghuang/vigogne-2-7b-instruct\", #ok pour les licenses\n","\n","    \"wizard7b_math\" : \"WizardLM/WizardMath-7B-V1.0\",\n","    \"wizard13b_math\" : \"WizardLM/WizardMath-13B-V1.0\",\n","\n","    \"wizard15b_coder\" : \"WizardLM/WizardCoder-15B-V1.0\",\n","    \"wizard34b_coder\" : \"WizardLM/WizardCoder-Python-34B-V1.0\",\n","\n","    ###bigscience bloom (7b)\n","    \"bloom7b\" : \"bigscience/bloom-7b1\",\n","    ## GPT-neo\n","    #\"gptNeo_original\" : \"EleutherAI/gpt-neo-2.7B\",\n","    ## GPT-J\n","    #\"gptJ_original\" : \"EleutherAI/gpt-j-6B\",\n","\n","}"]},{"cell_type":"markdown","metadata":{"id":"fqIr2Fwzw-kh"},"source":["Choix du modèle"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"GhlTgswddG7v"},"outputs":[],"source":["###### Choose your model with its name ######\n","model_name = \"mistral7b_instruct\" #@param [\"mistral7b_instruct\", \"mistral7b_orca\", \"zephyr7b\", \"vigostral7b\",\"bloom7b\", \"vigogne7b_instruct\", \"llama2-chat7b\", \"llama2-chat13b\", \"vigogne7b\",\"wizard7b_math\", \"wizard13b_math\", \"wizard15b_coder\", \"wizard34b_coder\"]\n"]},{"cell_type":"markdown","metadata":{"id":"9iXb5XWAxAvC"},"source":["Choix de la configuration pour quantifier (i.e. réduire le poids en baissant la qualité de représentation des flotants) le modèle\n"]},{"cell_type":"markdown","metadata":{"id":"t8YrWRxP1vTS"},"source":["Pour le moment : seulement 4bits, les autres config devraient pas passer sur colab et ça va crash"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"hn0uUBQAdIBJ"},"outputs":[],"source":["###### Choose your quantization config ######\n","quant_config = \"4bits\" # @param [\"4bits\", \"8bits\", \"16bits\", \"32bits\"]\n"]},{"cell_type":"markdown","metadata":{"id":"cxw6KrgFxLyX"},"source":["Chargement du modèle"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":162,"referenced_widgets":["167a387a1e384fa1a8b1f3e773378fbd","746cd86ee36c492d908760f254bc8591","b770aa03749046baafb4ea44547192ac","282eb85162a5406daaaef9f79536ac44","fc2e54eadf754baa91e7ae96339c842f","c6659ace946b407a9549a4f604b71f55","9f79bc51e11742e28824fe572948c3f5","c15cb7c30b8d4e57a7aeb15c27ddc0e4","8cf2274c55264398b7085b64002809c9","51cbd9afeea4485d8ac39f38abe4e34a","584124e830f54108876bb5ba43e9dda0","3dc96ec244e94139abbeee95060929a7","64cd679c34804000b61f7b91a6ef88ee","2338422a74ee413086404cd2548e7f58","3737da4ea12c4bf2ad77dd36f9e92979","e89de6cc0364407d9355e8761a3d3300","fdc6730d5778432bb980016edae4774a","66a08e29d4384aab82feaf5e73d7486c","55157c554d3a4f8aa4e9757fb49626cc","8c19f5497ff24cea97215c090e18b41b","3e286149a60b4ca09b27c3ffde75307a","f73cc4ca55d64fdc9377dec685a12da0","93544ea2b6a04f13958e9314cdeea05e","e3e4816950544a8ca295fcfe6d0f4379","9fb27f108ecb4c7b9f787bc542c414b0","3f8f22ba4e224ce1bec351a7072bc6ac","a1f6f53a25904f49bc62cdd9223063b9","3a41734f698443f99f01c43d1d753254","30f2081d8a844e3baf6694b0f433b825","3e8f8a33d2b54e74ba185f25fa84ce66","8cb86dbb99444f17ac0d8d5460bccc6c","1ab83116fa5548a0b6379282362e0b23","54fba074a3dd46be8d2a154eb3cfbdcd","7bcbed0c9dc04163a3523c62bd6103bf","34d242f2df4e4f26a3b864fa5f6b9dec","92733967e0284ee7a8db2f1748b9a35f","995df5b2501642b1a3d1399670651c25","4c283ddfe62b48b1b3b3e788c7394d0c","ec80ccc565e24cbc9a12b143738b5457","cd0314faa9254f529ffc493d8e4bd031","a37765cf5bcc489688c9df575873f1fe","acea18cf376f4901a7ffd4b4b130d626","7b84fc3c71ba480c8dfece282ae50a86","c02ddd857e1e438e9466c9f51500419c"]},"id":"a1sNuH5Qxip3","outputId":"4ee45f33-2f90-484e-d590-13e50d8d4322"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/pie2023/pie-msxs-08-val/venv/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]},{"ename":"ImportError","evalue":"cannot import name 'BitsAndBytesConfig'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-ce48e9b3e971>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBitsAndBytesConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'BitsAndBytesConfig'"]}],"source":["import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","\n","model_id = models_id[model_name]\n","\n","# quantization to int4 (don't want to mess with \"device\" here, to be studied)\n","#4bit, 4 bits = 1/2 byte --> #paramsInB * 1/2 = RAM needed to load full model\n","if quant_config == \"4bits\":\n","    print(\"Loading model in 4bits\")\n","    #quant config\n","    bnb_config = BitsAndBytesConfig(\n","        load_in_4bit=True,\n","        load_in_8bit=False,\n","        #bnb_4bit_use_double_quant=True,\n","        bnb_4bit_quant_type=\"nf4\",\n","        bnb_4bit_compute_dtype=torch.float16\n","    )\n","    #Load quantized model\n","    model = AutoModelForCausalLM.from_pretrained(\n","                model_id,\n","                quantization_config=bnb_config,\n","                #device_map=\"auto\",\n","            )\n","    tokenizer = AutoTokenizer.from_pretrained(model_id)\n","\n","\n","# quantization to int8  (don't want to mess with \"device\" here, to be studied)\n","#8bit, 8 bits = 1 byte --> #paramsInB * 1 = RAM needed to load full model\n","elif  quant_config == \"8bits\":\n","    print(\"Loading model in 8bits\")\n","    #load quantized model\n","    model = AutoModelForCausalLM.from_pretrained(\n","                model_id,\n","                #device_map=\"auto\",\n","                load_in_8bit=True, # 8bits here\n","          )\n","    tokenizer = AutoTokenizer.from_pretrained(model_id)\n","\n","\n","\n","#half-precision, 16 bits = 2 bytes --> #paramsInB * 2 = RAM needed to load full model\n","elif  quant_config == \"16bits\":\n","    print(\"Loading model in half-precision\")\n","    #device-agnostic code\n","    device = torch.device(\n","                \"cuda\" if torch.cuda.is_available()\n","                else \"cpu\"\n","            )\n","    #load model\n","    model = AutoModelForCausalLM.from_pretrained(\n","                model_id,\n","                torch_dtype=torch.float16, #half-precision here\n","                device_map=\"auto\",\n","            )#.to(device)\n","    tokenizer = AutoTokenizer.from_pretrained(model_id)#.to(device)\n","\n","\n","\n","#full-precision, 32bits = 4 bytes --> #paramsInB * 4 = RAM needed to load full model\n","elif  quant_config == \"32bits\":\n","    print(\"Loading model in full-precision\")\n","    #device-agnostic code\n","    device = torch.device(\n","                \"cuda\" if torch.cuda.is_available()\n","                else \"cpu\"\n","            )\n","    #load model\n","    model = AutoModelForCausalLM.from_pretrained(\n","                model_id,\n","                torch_dtype=torch.float32, #full-precision here\n","            ).to(device)\n","    tokenizer = AutoTokenizer.from_pretrained(model_id)#.to(device)\n"]},{"cell_type":"markdown","metadata":{"id":"etu-SX03xNow"},"source":["Let's goo : basic prompting with/without instruction depending on the model (chat or not)"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"s0Ft1ktvkMRr"},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m/home/pie2023/pie-msxs-08/notebooks/playground.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.66/home/pie2023/pie-msxs-08/notebooks/playground.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m text_input \u001b[39m=\u001b[39m instruction \u001b[39m+\u001b[39m prompt \u001b[39mif\u001b[39;00m chat \u001b[39melse\u001b[39;00m prompt_no_chat\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.66/home/pie2023/pie-msxs-08/notebooks/playground.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m inputs \u001b[39m=\u001b[39m tokenizer(text_input, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mto(model\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.66/home/pie2023/pie-msxs-08/notebooks/playground.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mgenerate(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.66/home/pie2023/pie-msxs-08/notebooks/playground.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.66/home/pie2023/pie-msxs-08/notebooks/playground.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39m#temperature=1.1, # >1 augmente la diversité/surprise sur la génération (applatie la distribution sur le next token), <1 diminue la diversité de la génération (rend la distribution + spiky)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.66/home/pie2023/pie-msxs-08/notebooks/playground.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     do_sample\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.66/home/pie2023/pie-msxs-08/notebooks/playground.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     top_k\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.66/home/pie2023/pie-msxs-08/notebooks/playground.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m     top_p\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, \u001b[39m# le token suivant est tiré du top 'top_p' de la distribution uniquement\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.66/home/pie2023/pie-msxs-08/notebooks/playground.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m     num_return_sequences\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.66/home/pie2023/pie-msxs-08/notebooks/playground.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     repetition_penalty\u001b[39m=\u001b[39m\u001b[39m1.5\u001b[39m, \u001b[39m#pour éviter les répétitions, je suis pas au clair avec commment il marche celui-là mais important à priori\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.66/home/pie2023/pie-msxs-08/notebooks/playground.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m     eos_token_id\u001b[39m=\u001b[39mtokenizer\u001b[39m.\u001b[39meos_token_id,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.66/home/pie2023/pie-msxs-08/notebooks/playground.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m     max_length\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.66/home/pie2023/pie-msxs-08/notebooks/playground.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.66/home/pie2023/pie-msxs-08/notebooks/playground.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# display the generated answer\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.66/home/pie2023/pie-msxs-08/notebooks/playground.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mprint\u001b[39m(tokenizer\u001b[39m.\u001b[39mdecode(outputs[\u001b[39m0\u001b[39m], skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n","File \u001b[0;32m~/miniconda3/envs/llm_playground/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","File \u001b[0;32m~/miniconda3/envs/llm_playground/lib/python3.11/site-packages/transformers/generation/utils.py:1800\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1792\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1793\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1794\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1795\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1796\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1797\u001b[0m     )\n\u001b[1;32m   1799\u001b[0m     \u001b[39m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1800\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample(\n\u001b[1;32m   1801\u001b[0m         input_ids,\n\u001b[1;32m   1802\u001b[0m         logits_processor\u001b[39m=\u001b[39mlogits_processor,\n\u001b[1;32m   1803\u001b[0m         logits_warper\u001b[39m=\u001b[39mlogits_warper,\n\u001b[1;32m   1804\u001b[0m         stopping_criteria\u001b[39m=\u001b[39mstopping_criteria,\n\u001b[1;32m   1805\u001b[0m         pad_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mpad_token_id,\n\u001b[1;32m   1806\u001b[0m         eos_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39meos_token_id,\n\u001b[1;32m   1807\u001b[0m         output_scores\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39moutput_scores,\n\u001b[1;32m   1808\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mreturn_dict_in_generate,\n\u001b[1;32m   1809\u001b[0m         synced_gpus\u001b[39m=\u001b[39msynced_gpus,\n\u001b[1;32m   1810\u001b[0m         streamer\u001b[39m=\u001b[39mstreamer,\n\u001b[1;32m   1811\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1812\u001b[0m     )\n\u001b[1;32m   1814\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1815\u001b[0m     \u001b[39m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1816\u001b[0m     beam_scorer \u001b[39m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1817\u001b[0m         batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1818\u001b[0m         num_beams\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1823\u001b[0m         max_length\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mmax_length,\n\u001b[1;32m   1824\u001b[0m     )\n","File \u001b[0;32m~/miniconda3/envs/llm_playground/lib/python3.11/site-packages/transformers/generation/utils.py:2897\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2894\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2896\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2897\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(\n\u001b[1;32m   2898\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_inputs,\n\u001b[1;32m   2899\u001b[0m     return_dict\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   2900\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m   2901\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   2902\u001b[0m )\n\u001b[1;32m   2904\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2905\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/llm_playground/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/miniconda3/envs/llm_playground/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1037\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1034\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m   1036\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1037\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(\n\u001b[1;32m   1038\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1039\u001b[0m     attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[1;32m   1040\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   1041\u001b[0m     past_key_values\u001b[39m=\u001b[39mpast_key_values,\n\u001b[1;32m   1042\u001b[0m     inputs_embeds\u001b[39m=\u001b[39minputs_embeds,\n\u001b[1;32m   1043\u001b[0m     use_cache\u001b[39m=\u001b[39muse_cache,\n\u001b[1;32m   1044\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m   1045\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1046\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m   1047\u001b[0m )\n\u001b[1;32m   1049\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1050\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpretraining_tp \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n","File \u001b[0;32m~/miniconda3/envs/llm_playground/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/miniconda3/envs/llm_playground/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:925\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    915\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    916\u001b[0m         decoder_layer\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m    917\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    922\u001b[0m         use_cache,\n\u001b[1;32m    923\u001b[0m     )\n\u001b[1;32m    924\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 925\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m    926\u001b[0m         hidden_states,\n\u001b[1;32m    927\u001b[0m         attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[1;32m    928\u001b[0m         position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m    929\u001b[0m         past_key_value\u001b[39m=\u001b[39mpast_key_value,\n\u001b[1;32m    930\u001b[0m         output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m    931\u001b[0m         use_cache\u001b[39m=\u001b[39muse_cache,\n\u001b[1;32m    932\u001b[0m     )\n\u001b[1;32m    934\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    936\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n","File \u001b[0;32m~/miniconda3/envs/llm_playground/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/miniconda3/envs/llm_playground/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:689\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    688\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 689\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp(hidden_states)\n\u001b[1;32m    690\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n\u001b[1;32m    692\u001b[0m outputs \u001b[39m=\u001b[39m (hidden_states,)\n","File \u001b[0;32m~/miniconda3/envs/llm_playground/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/miniconda3/envs/llm_playground/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:261\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    259\u001b[0m     down_proj \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(down_proj)\n\u001b[1;32m    260\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 261\u001b[0m     down_proj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdown_proj(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact_fn(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgate_proj(x)) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mup_proj(x))\n\u001b[1;32m    263\u001b[0m \u001b[39mreturn\u001b[39;00m down_proj\n","File \u001b[0;32m~/miniconda3/envs/llm_playground/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/miniconda3/envs/llm_playground/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mlinear(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Instruction : if the model is a chat model, specify context, persona, personality, skills, ...\n","\n","###\n","chat = True\n","###\n","\n","instruction = \"You are a passionate elementary school teacher. \" + \\\n","\"You are teaching a class of 20 pupils. \" + \\\n","\"You love to explain things to childrens with images they understand at their age and relevant examples.\"\n","\n","# Prompt : your question, task, ...\n","prompt = \"Write a math exercice around a football with a couple of multiplications.\"\n","prompt_no_chat = \"Here is a small 3-examples math word problem for children aged 8 years old on basic multiplication with a football theme/story to hook them : \"\n","\n","text_input = instruction + prompt if chat else prompt_no_chat\n","inputs = tokenizer(text_input, return_tensors=\"pt\").to(model.device)\n","\n","\n","outputs = model.generate(\n","    **inputs,\n","    #temperature=1.1, # >1 augmente la diversité/surprise sur la génération (applatie la distribution sur le next token), <1 diminue la diversité de la génération (rend la distribution + spiky)\n","    do_sample=False,\n","    top_k=5,\n","    top_p=10, # le token suivant est tiré du top 'top_p' de la distribution uniquement\n","    num_return_sequences=1,\n","    repetition_penalty=1.5, #pour éviter les répétitions, je suis pas au clair avec commment il marche celui-là mais important à priori\n","    eos_token_id=tokenizer.eos_token_id,\n","    max_length=1024,\n","    )\n","\n","# display the generated answer\n","print(tokenizer.decode(outputs[0], skip_special_tokens=True))"]},{"cell_type":"markdown","metadata":{"id":"WdfHvtYbmJBQ"},"source":["## Trying GGUF models, should be lighter and work well"]},{"cell_type":"markdown","metadata":{"id":"daC2ZM1FIPyV"},"source":["Résumer des besoins hardware\n","\n","- https://www.hardware-corner.net/llm-database/Vigogne/"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19529,"status":"ok","timestamp":1699783094766,"user":{"displayName":"Nicolas PELLERIN","userId":"16955010145826763446"},"user_tz":-60},"id":"9qP_mFmVmY4W","outputId":"3f7f99e3-4f5d-4cdf-bdc5-0daa087394cd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/.shortcut-targets-by-id/15XKDUxg701tiQkxuN3G7q6MCvcYEFp44/PIE - MSXS-08/4-LLMs\n"]}],"source":["# On colab\n","on_colab = False\n","if on_colab:\n","  from google.colab import drive\n","  drive.mount('/content/drive')\n","  %cd \"/content/drive/MyDrive/PIE - MSXS-08/4-LLMs/\""]},{"cell_type":"markdown","metadata":{"id":"Id7ubTutxxF_"},"source":["-  https://huggingface.co/TheBloke/Mistral-7B-v0.1-GGUF\n","- https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF\n"]},{"cell_type":"markdown","metadata":{"id":"HbRPNdlVxzYG"},"source":["### 1. On télécharge les poids du modèle + config qui nous intérèsse.\n","\n","Par exemple un mistral7b quantifié"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":91373,"status":"ok","timestamp":1699730434586,"user":{"displayName":"Nicolas PELLERIN","userId":"16955010145826763446"},"user_tz":-60},"id":"jt7Oh-04mHk3","outputId":"bdcbc466-acf7-44f7-99ee-1d83d060fbc7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (0.19.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (3.13.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (4.66.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (4.5.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (23.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (2023.7.22)\n","Consider using `hf_transfer` for faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\n","downloading https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf to /root/.cache/huggingface/hub/tmpc6ntp7mm\n","llama-2-7b-chat.Q4_K_M.gguf: 100% 4.08G/4.08G [00:38<00:00, 106MB/s] \n","./llama-2-7b-chat.Q4_K_M.gguf\n"]}],"source":["## 1. download model weights locally\n","\n","#!pip3 install -q huggingface-hub\n","#!huggingface-cli download TheBloke/Llama-2-7b-Chat-GGUF llama-2-7b-chat.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1699783094767,"user":{"displayName":"Nicolas PELLERIN","userId":"16955010145826763446"},"user_tz":-60},"id":"ruKP1aXc4xSv","outputId":"351898c1-aff7-47a3-fd9b-854c38b1a2e7"},"outputs":[{"name":"stdout","output_type":"stream","text":["playground.ipynb  test.py\n"]}],"source":["!cd /home/pie2023/pie-msxs-08-val/models\n","!ls"]},{"cell_type":"markdown","metadata":{"id":"VGXniGCTx-92"},"source":["On charge le modèle avec ctransformers, une librairie python pour utiliser des modèles styles ggml/gguf qu'on utilise normalement avec du c/c++"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11403,"status":"ok","timestamp":1699783075243,"user":{"displayName":"Nicolas PELLERIN","userId":"16955010145826763446"},"user_tz":-60},"id":"X2VoMD1hyHlj","outputId":"1ede9ac6-0fbe-4107-c9c5-b19b165ba0e4"},"outputs":[],"source":["# 2. load the downloaded model with ctransformers\n","#Base ctransformers with no GPU acceleration\n","!pip install -q ctransformers\n","##Or with CUDA GPU acceleration\n","#pip install ctransformers[cuda]"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"BYuh81BslJqT"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/pie2023/miniconda3/envs/llm_playground/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","config.json: 100%|██████████| 31.0/31.0 [00:00<00:00, 204kB/s]\n","Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00,  3.32it/s]\n","mistral-7b-v0.1.Q4_K_M.gguf: 100%|██████████| 4.37G/4.37G [00:40<00:00, 108MB/s]\n","Fetching 1 files: 100%|██████████| 1/1 [00:40<00:00, 40.64s/it]\n"]},{"ename":"OSError","evalue":"/lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /home/pie2023/miniconda3/envs/llm_playground/lib/python3.11/site-packages/ctransformers/lib/avx2/libctransformers.so)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[1;32m/home/pie2023/pie-msxs-08-val/notebooks/playground.ipynb Cell 25\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.66/home/pie2023/pie-msxs-08-val/notebooks/playground.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mctransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoModelForCausalLM\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.66/home/pie2023/pie-msxs-08-val/notebooks/playground.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.66/home/pie2023/pie-msxs-08-val/notebooks/playground.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m llm \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.66/home/pie2023/pie-msxs-08-val/notebooks/playground.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mTheBloke/Mistral-7B-v0.1-GGUF\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.66/home/pie2023/pie-msxs-08-val/notebooks/playground.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     model_file\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmistral-7b-v0.1.Q4_K_M.gguf\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.66/home/pie2023/pie-msxs-08-val/notebooks/playground.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     model_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmistral\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.66/home/pie2023/pie-msxs-08-val/notebooks/playground.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     gpu_layers\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.66/home/pie2023/pie-msxs-08-val/notebooks/playground.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.66/home/pie2023/pie-msxs-08-val/notebooks/playground.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m#demo\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.66/home/pie2023/pie-msxs-08-val/notebooks/playground.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m llm(\u001b[39m\"\u001b[39m\u001b[39mAI is going to\u001b[39m\u001b[39m\"\u001b[39m, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n","File \u001b[0;32m~/miniconda3/envs/llm_playground/lib/python3.11/site-packages/ctransformers/hub.py:175\u001b[0m, in \u001b[0;36mAutoModelForCausalLM.from_pretrained\u001b[0;34m(cls, model_path_or_repo_id, model_type, model_file, config, lib, local_files_only, revision, hf, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39melif\u001b[39;00m path_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrepo\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    168\u001b[0m     model_path \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_find_model_path_from_repo(\n\u001b[1;32m    169\u001b[0m         model_path_or_repo_id,\n\u001b[1;32m    170\u001b[0m         model_file,\n\u001b[1;32m    171\u001b[0m         local_files_only\u001b[39m=\u001b[39mlocal_files_only,\n\u001b[1;32m    172\u001b[0m         revision\u001b[39m=\u001b[39mrevision,\n\u001b[1;32m    173\u001b[0m     )\n\u001b[0;32m--> 175\u001b[0m llm \u001b[39m=\u001b[39m LLM(\n\u001b[1;32m    176\u001b[0m     model_path\u001b[39m=\u001b[39mmodel_path,\n\u001b[1;32m    177\u001b[0m     model_type\u001b[39m=\u001b[39mmodel_type,\n\u001b[1;32m    178\u001b[0m     config\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mconfig,\n\u001b[1;32m    179\u001b[0m     lib\u001b[39m=\u001b[39mlib,\n\u001b[1;32m    180\u001b[0m )\n\u001b[1;32m    181\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m hf:\n\u001b[1;32m    182\u001b[0m     \u001b[39mreturn\u001b[39;00m llm\n","File \u001b[0;32m~/miniconda3/envs/llm_playground/lib/python3.11/site-packages/ctransformers/llm.py:246\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model_path, model_type, config, lib)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    241\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUnable to detect model type. Please specify a model type using:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m  AutoModelForCausalLM.from_pretrained(..., model_type=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m...\u001b[39m\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m         )\n\u001b[1;32m    244\u001b[0m     model_type \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgguf\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 246\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lib \u001b[39m=\u001b[39m load_library(lib, gpu\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mgpu_layers \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[1;32m    247\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_llm \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lib\u001b[39m.\u001b[39mctransformers_llm_create(\n\u001b[1;32m    248\u001b[0m     model_path\u001b[39m.\u001b[39mencode(),\n\u001b[1;32m    249\u001b[0m     model_type\u001b[39m.\u001b[39mencode(),\n\u001b[1;32m    250\u001b[0m     config\u001b[39m.\u001b[39mto_struct(),\n\u001b[1;32m    251\u001b[0m )\n\u001b[1;32m    252\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_llm \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","File \u001b[0;32m~/miniconda3/envs/llm_playground/lib/python3.11/site-packages/ctransformers/llm.py:126\u001b[0m, in \u001b[0;36mload_library\u001b[0;34m(path, gpu)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m path:\n\u001b[1;32m    125\u001b[0m     load_cuda()\n\u001b[0;32m--> 126\u001b[0m lib \u001b[39m=\u001b[39m CDLL(path)\n\u001b[1;32m    128\u001b[0m lib\u001b[39m.\u001b[39mctransformers_llm_create\u001b[39m.\u001b[39margtypes \u001b[39m=\u001b[39m [\n\u001b[1;32m    129\u001b[0m     c_char_p,  \u001b[39m# model_path\u001b[39;00m\n\u001b[1;32m    130\u001b[0m     c_char_p,  \u001b[39m# model_type\u001b[39;00m\n\u001b[1;32m    131\u001b[0m     ConfigStruct,  \u001b[39m# config\u001b[39;00m\n\u001b[1;32m    132\u001b[0m ]\n\u001b[1;32m    133\u001b[0m lib\u001b[39m.\u001b[39mctransformers_llm_create\u001b[39m.\u001b[39mrestype \u001b[39m=\u001b[39m llm_p\n","File \u001b[0;32m~/miniconda3/envs/llm_playground/lib/python3.11/ctypes/__init__.py:376\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_FuncPtr \u001b[39m=\u001b[39m _FuncPtr\n\u001b[1;32m    375\u001b[0m \u001b[39mif\u001b[39;00m handle \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 376\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle \u001b[39m=\u001b[39m _dlopen(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name, mode)\n\u001b[1;32m    377\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    378\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle \u001b[39m=\u001b[39m handle\n","\u001b[0;31mOSError\u001b[0m: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /home/pie2023/miniconda3/envs/llm_playground/lib/python3.11/site-packages/ctransformers/lib/avx2/libctransformers.so)"]}],"source":["from ctransformers import AutoModelForCausalLM\n","# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n","llm = AutoModelForCausalLM.from_pretrained(\n","    \"TheBloke/Mistral-7B-v0.1-GGUF\",\n","    model_file=\"mistral-7b-v0.1.Q4_K_M.gguf\",\n","    model_type=\"mistral\",\n","    \n","    gpu_layers=0\n",")\n","\n","#demo\n","for text in llm(\"AI is going to\", stream=True):\n","    print(text, end=\"\", flush=True)\n"]},{"cell_type":"markdown","metadata":{"id":"NXZOP__tyJMG"},"source":["On génère du texte. Je vais voir pour rajouter des config sur la génération pour mieux la contrôler"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":245103,"status":"ok","timestamp":1699783521536,"user":{"displayName":"Nicolas PELLERIN","userId":"16955010145826763446"},"user_tz":-60},"id":"wwpKAXcTtfh1","outputId":"1c32b798-5720-4926-a44d-044d96e8e42f"},"outputs":[{"name":"stdout","output_type":"stream","text":["2 players are playing football . One scores two goals\n","\n"," and the other scores three goals. How many goals\n","\n"," were scored? The answer is 5 goals and\n","\n"," it’s so easy to make and write!\n","\n","\n","\n","This is an example of the problems found in\n","\n"," a Football Math Workbook for children learning the basic\n","\n"," multiplications.\n","\n","There are over 7\n","\n","0 word problems in this workbook. All of\n","\n"," them are related to football with fun illustrations to\n","\n"," keep kids interested. Children will learn multiplication while\n","\n"," they improve their reading comprehension skills. The answers\n","\n"," are at the end of each page, so you\n","\n"," can check your child’s work and give instant feedback\n","\n"," on his or her performance.\n","\n","We do\n","\n"," sell this book in the shop as a paperback\n","\n"," or PDF copy which means you have immediate access to\n","\n"," it. You can also read a sample of some\n","\n"," pages for free from our blog posts . We do\n","\n"," offer a 30 day money back guarantee if\n","\n"," you are not satisfied with your purchase."]}],"source":["prompt_no_chat = \"Here is a math word problem for children learning basic multiplications with a football theme/story to hook them : \"\n","\n","tok=0\n","for text in llm(prompt_no_chat, stream=True):\n","    print(text, end=\"\", flush=True)\n","\n","    # for visibility on colab\n","    tok+=1\n","    if tok % 10 == 0:\n","      print(\"\\n\")\n"]},{"cell_type":"markdown","metadata":{"id":"n05MROgf_CsD"},"source":["### llama2 7b int4:"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["672da5a5a040476dbb248c68e62860ba","ad5ece063ec643d59221e4f6ee82894f","dbd8f51817634ceaaebe2028f5a48cfb","bc077fe5da564f358bf4d321a3792db5","176e1765d8f6468d9b8d6428aadef114","82c07a4dffec43c78f3a2b9a28be82a0","ec781af4f26d479ea9e9444ce817f6ab","4f7287391fbf48cdbd7a34608de3eb79","9240208765784ec796b30d65ca315027","2b78a10b1cd641229bc9aa06cf9c6de9","c68e4d0d844a4178b5f7114c0a0074c1","2229e28846a64eb7affae38e2f252bcd","8f58e1eebfe746adad6b63b6fba41775","fa83598231a1416ca14620fbbc870431","eeb3e01b667b46aeae4671434a8aef69","1c3dab9bd34f47e4add52a03c4b98290","df97d6f6f2b841b4b6a0ed3c35a81c35","cab7cd1994d54e3aa817795a4746151e","bfaaa2ed2d0442d7a6e010054fed156f","dd74fcee8211464ea7b5448eb537551c","7ae6d85a7fe44bd98978c013094ac232","bc048c2f991b494299a3c911a379765c","8c70d57d11e241a680ca775f84604ec6","06148f89071241fd9d8f4bef6e75ff45","1e5475c5ab9e4cc7b300de85b4ef03e7","971a02bd869e43ccb3f843bafbe4f3dd","40ceb6510ee8402eaaa88fe0062b80c3","d16760c760ac47c3b262997a753d571c","f3868faf727c4373a4accc9c0448987b","c0bb975b80b04f628b8af5ff6877b2a1","c9e0a2b2628842d3a9a9f2e5d4b7b735","372e18d0f38d455fb091c491ae2528f8","ef86517fd1cd427cbe77c38601a674d6","373d05ac765649b9a90463c6281fe42c","41ab6d5da2ce4365bb720e4d212b30f6","def28959529342c4856b3e10edf94304","a9d70c251a204380953240c4e2c04081","39083b399202429c88555ebea273dd48","c8cbe59ae47d4227b72a64a998fb71ef","da79dac112ce4ee9b1b6c06844c775de","7fa8ceff87bc40e4883073b16bc8a981","47b60d282dd44924a03abe17dc13eb79","f3f9e704932443f19ea0f5ced0c09cc8","9dda30b69fb34c9b9a2ce54776bbccf1"]},"executionInfo":{"elapsed":98382,"status":"ok","timestamp":1699784134797,"user":{"displayName":"Nicolas PELLERIN","userId":"16955010145826763446"},"user_tz":-60},"id":"hQrZ9dJp1OzS","outputId":"4bd35353-0a4f-41c0-ce7f-f913e03871c2"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"672da5a5a040476dbb248c68e62860ba","version_major":2,"version_minor":0},"text/plain":["Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2229e28846a64eb7affae38e2f252bcd","version_major":2,"version_minor":0},"text/plain":["(…)2882fb562ffccdd1cf0f65402adb/config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8c70d57d11e241a680ca775f84604ec6","version_major":2,"version_minor":0},"text/plain":["Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"373d05ac765649b9a90463c6281fe42c","version_major":2,"version_minor":0},"text/plain":["llama-2-7b-chat.Q4_K_M.gguf:   0%|          | 0.00/4.08G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from ctransformers import AutoModelForCausalLM\n","# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n","llm = AutoModelForCausalLM.from_pretrained(\n","   \"TheBloke/Llama-2-7b-Chat-GGUF\",\n","   model_file=\"llama-2-7b-chat.Q4_K_M.gguf\",\n","   model_type=\"llama\",\n","   gpu_layers=0,\n","  )"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":230592,"status":"ok","timestamp":1699784365374,"user":{"displayName":"Nicolas PELLERIN","userId":"16955010145826763446"},"user_tz":-60},"id":"BxCNIo6f_B-7","outputId":"fcb7d833-adaf-4452-cc81-88cfc6a4e4c8"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","Example 1: Sarah has 5 footballs and she wants to know how many she will have if she buys 3 more. Can you help her calculate the answer?\n","Example 2: Tom has 8 players on his football team, and each player needs 4 water bottles. How many water bottles does Tom need in total?\n","Example 3: If a football field is 100 yards long, and a player runs 50 yards in one direction, how far does the player run in total?\n","\n","Answer key for above example:\n","For Example 1: Sarah will have 8 footballs if she buys 3 more.\n","For Example 2: Tom needs 32 water bottles in total.\n","For Example 3: The player runs a total of 100 yards."]}],"source":["prompt_no_chat = \"Here is a small 3-examples math word problem for children aged 8 years old on basic multiplication with a football theme/story to hook them : \"\n","\n","for text in llm(prompt_no_chat, stream=True):\n","    print(text, end=\"\", flush=True)"]},{"cell_type":"markdown","metadata":{"id":"lLDfssOIB6MS"},"source":["### llama2-13b-chat int4\n","\n","- https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7hMuAnO9CGJ4"},"outputs":[],"source":["# ne fonctionne pas pour une raison étrange, les adresses ont pourtant l'air correcte\n","#!huggingface-cli download TheBloke/Llama-2-13B-chat-GGUF llama-2-13b-chat.q4_K_M.gguf --local-dir . --local-dir-use-symlinks False"]},{"cell_type":"markdown","metadata":{"id":"qddXuyk1GKa9"},"source":["trying with gptq instead"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OLxV2eriF4LT"},"outputs":[],"source":["!pip3 install transformers>=4.32.0 optimum>=1.12.0\n","!pip3 install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/  # Use cu117 if on CUDA 11.7\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M0lX56LxFrCe"},"outputs":[],"source":["from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n","\n","model_name_or_path = \"TheBloke/Llama-2-13B-chat-GPTQ\"\n","# To use a different branch, change revision\n","# For example: revision=\"main\"\n","model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n","                                             device_map=\"auto\",\n","                                             trust_remote_code=False,\n","                                             revision=\"main\")\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n","\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29812,"status":"ok","timestamp":1699786756903,"user":{"displayName":"Nicolas PELLERIN","userId":"16955010145826763446"},"user_tz":-60},"id":"qO6iDhKvJq2V","outputId":"fd7487a6-2c23-4208-acc3-82f15eae4190"},"outputs":[{"name":"stdout","output_type":"stream","text":["[INST] <<SYS>>\n","You are a passionate and honest elementary school teacher. You are teaching a class of 20 pupils. You love to explain things to childrens with images they understand at their age and relevant examples. Please help with the following task.\n","<</SYS>>\n","Please write a math exercice for 10-year-old children on basic multiplications with small digits with a football theme. I need 4 short examples of multiplications.[/INST]\n","\n","Hey there, young mathematicians! Today, we're going to kick off our math lesson with a fun football theme! 🏈👋\n","\n","Example 1: Score a Goal! 🏆\n","Imagine you're playing soccer, and you score a goal! If you scored that goal using your left foot, how many times do you think you kicked the ball with your left foot? 🤔\n","\n","That's right, 3 times! 🙌 And if you kicked the ball 3 times with your left foot, and each kick was 5 feet apart, how far did you kick the ball in total? 🤝\n","\n","That's 15 feet! 😮 Can you imagine kicking the ball that far? Wow!\n","\n","Example 2: Pass the Ball! 🏃‍♀️\n","Now, imagine you're passing the ball to your teammate. If you passed the ball 4 times, and each pass was 8 feet long, how far did you pass the ball in total? 🤔\n","\n","That's 32 feet! 😲 Whoa, that's a long pass!\n","\n","Example 3: Run Like Lightning! 🐯\n","Imagine you're running as fast as lightning during a game. If you ran 6 laps around the field, and each lap was 100 feet long, how far did you run in total? 🤯\n","\n","That's 600 feet! 😱 Whoa, that's like running from here to the park and back again!\n","\n","Example 4: Shoot for the Stars! 🚀\n","Finally, imagine you're shooting a penalty kick into the goal. If you shot the ball 2 times, and each shot was 15 feet high, how high did you shoot the ball in total? 🤔\n","\n","That's 30 feet! 🚀 Woah, that's almost as high as the goalposts!\n","\n","So, let's review what we learned today:\n","\n","* 3 x 5 = 15 (left foot kicks)\n","* 4 x 8 = 32\n"]}],"source":["prompt = \"Please write a math exercice for 10-year-old children on basic multiplications with small digits with a football theme. I need 4 short examples of multiplications.\"\n","prompt_template=f'''[INST] <<SYS>>\n","You are a passionate and honest elementary school teacher. You are teaching a class of 20 pupils. You love to explain things to childrens with images they understand at their age and relevant examples. Please help with the following task.\n","<</SYS>>\n","{prompt}[/INST]\n","\n","'''\n","\n","\n","pipe = pipeline(\n","    \"text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    max_new_tokens=512,\n","    do_sample=True,\n","    temperature=0.7,\n","    top_p=0.95,\n","    top_k=40,\n","    repetition_penalty=1.1\n",")\n","\n","print(pipe(prompt_template)[0]['generated_text'])"]},{"cell_type":"markdown","metadata":{"id":"YRhNh1ek0cVb"},"source":["## More advanced prompting :prompt templates using LangChain for chat systems"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gd6NIfE70f2A"},"outputs":[],"source":["from transformers import pipeline\n","from langchain import HuggingFacePipeline\n","from langchain import PromptTemplate, LLMChain\n","\n","\n","hf_pipeline = pipeline(\n","        \"text-generation\",\n","        model=model,\n","        tokenizer=tokenizer,\n","        use_cache=True,\n","        device_map=\"auto\",\n","        max_length=500,\n","        do_sample=True,\n","        top_k=5,\n","        num_return_sequences=1,\n","        eos_token_id=tokenizer.eos_token_id,\n","        pad_token_id=tokenizer.eos_token_id,\n",")\n","llm = HuggingFacePipeline(pipeline=hf_pipeline)\n","\n","\n","#### Prompt\n","template = \"\"\"<s>[INST] \"You are a passionate elementary school teacher.\n","You are teaching a class of 20 pupils.\n","You love to explain things to childrens with images they understand at their age and relevant examples. Please help with the following task.\n","{context}\n","{question} [/INST] </s>\n","\"\"\"\n","\n","question_p = \"\"\"Writte a math problem around a football story to train them at multiplying small digits together. I need 5 examples.\"\"\"\n","context_p = \"\"\" You are teahcing a class of a dozen 10 year-old children.\"\"\"\n","prompt = PromptTemplate(template=template, input_variables=[\"question\",\"context\"])\n","\n","llm_chain = LLMChain(prompt=prompt, llm=llm)\n","response = llm_chain.run({\"question\":question_p,\"context\":context_p})\n","response"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qlCGI6nexp6S"},"outputs":[],"source":["!pip install awq"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SNIKbyuAxnbd"},"outputs":[],"source":["from awq import AutoAWQForCausalLM\n","from transformers import AutoTokenizer\n","\n","model_name_or_path = \"TheBloke/Llama-2-7b-Chat-AWQ\"\n","\n","# Load model\n","model = AutoAWQForCausalLM.from_quantized(model_name_or_path, fuse_layers=True,\n","                                          trust_remote_code=False, safetensors=True)\n","tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=False)\n","\n","prompt = \"Tell me about AI\"\n","prompt_template=f'''[INST] <<SYS>>\n","You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n","<</SYS>>\n","{prompt}[/INST]\n","\n","'''\n","\n","print(\"\\n\\n*** Generate:\")\n","\n","tokens = tokenizer(\n","    prompt_template,\n","    return_tensors='pt'\n",").input_ids.cuda()\n","\n","# Generate output\n","generation_output = model.generate(\n","    tokens,\n","    do_sample=True,\n","    temperature=0.7,\n","    top_p=0.95,\n","    top_k=40,\n","    max_new_tokens=512\n",")\n","\n","print(\"Output: \", tokenizer.decode(generation_output[0]))\n","\n","# Inference can also be done using transformers' pipeline\n","from transformers import pipeline\n","\n","print(\"*** Pipeline:\")\n","pipe = pipeline(\n","    \"text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    max_new_tokens=512,\n","    do_sample=True,\n","    temperature=0.7,\n","    top_p=0.95,\n","    top_k=40,\n","    repetition_penalty=1.1\n",")\n","\n","print(pipe(prompt_template)[0]['generated_text'])\n"]},{"cell_type":"markdown","metadata":{"id":"oObKDMWtuPCV"},"source":["## Llama.cpp"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V0JcPUqbuQOs"},"outputs":[],"source":["#!git clone https://github.com/ggerganov/llama.cpp.git\n","#!(cd llama.cpp; make)\n","#!llama.cpp/main -m models/llama-13b-v2/ggml-model-q4_0.gguf -p \"Building a website can be done in 10 simple steps:\\nStep 1:\" -n 400 -e\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["HbRPNdlVxzYG","n05MROgf_CsD"],"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"widgets":{"application/vnd.jupyter.widget-state+json":{"06148f89071241fd9d8f4bef6e75ff45":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d16760c760ac47c3b262997a753d571c","placeholder":"​","style":"IPY_MODEL_f3868faf727c4373a4accc9c0448987b","value":"Fetching 1 files: 100%"}},"167a387a1e384fa1a8b1f3e773378fbd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_746cd86ee36c492d908760f254bc8591","IPY_MODEL_b770aa03749046baafb4ea44547192ac","IPY_MODEL_282eb85162a5406daaaef9f79536ac44"],"layout":"IPY_MODEL_fc2e54eadf754baa91e7ae96339c842f"}},"176e1765d8f6468d9b8d6428aadef114":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ab83116fa5548a0b6379282362e0b23":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c3dab9bd34f47e4add52a03c4b98290":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e5475c5ab9e4cc7b300de85b4ef03e7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c0bb975b80b04f628b8af5ff6877b2a1","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c9e0a2b2628842d3a9a9f2e5d4b7b735","value":1}},"2229e28846a64eb7affae38e2f252bcd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8f58e1eebfe746adad6b63b6fba41775","IPY_MODEL_fa83598231a1416ca14620fbbc870431","IPY_MODEL_eeb3e01b667b46aeae4671434a8aef69"],"layout":"IPY_MODEL_1c3dab9bd34f47e4add52a03c4b98290"}},"2338422a74ee413086404cd2548e7f58":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_55157c554d3a4f8aa4e9757fb49626cc","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8c19f5497ff24cea97215c090e18b41b","value":1}},"282eb85162a5406daaaef9f79536ac44":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_51cbd9afeea4485d8ac39f38abe4e34a","placeholder":"​","style":"IPY_MODEL_584124e830f54108876bb5ba43e9dda0","value":" 23.9k/23.9k [00:00&lt;00:00, 314kB/s]"}},"2b78a10b1cd641229bc9aa06cf9c6de9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"30f2081d8a844e3baf6694b0f433b825":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"34d242f2df4e4f26a3b864fa5f6b9dec":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ec80ccc565e24cbc9a12b143738b5457","placeholder":"​","style":"IPY_MODEL_cd0314faa9254f529ffc493d8e4bd031","value":"Downloading (…)l-00002-of-00002.bin:  33%"}},"372e18d0f38d455fb091c491ae2528f8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3737da4ea12c4bf2ad77dd36f9e92979":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3e286149a60b4ca09b27c3ffde75307a","placeholder":"​","style":"IPY_MODEL_f73cc4ca55d64fdc9377dec685a12da0","value":" 1/2 [01:57&lt;01:57, 117.91s/it]"}},"373d05ac765649b9a90463c6281fe42c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_41ab6d5da2ce4365bb720e4d212b30f6","IPY_MODEL_def28959529342c4856b3e10edf94304","IPY_MODEL_a9d70c251a204380953240c4e2c04081"],"layout":"IPY_MODEL_39083b399202429c88555ebea273dd48"}},"39083b399202429c88555ebea273dd48":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3a41734f698443f99f01c43d1d753254":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3dc96ec244e94139abbeee95060929a7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_64cd679c34804000b61f7b91a6ef88ee","IPY_MODEL_2338422a74ee413086404cd2548e7f58","IPY_MODEL_3737da4ea12c4bf2ad77dd36f9e92979"],"layout":"IPY_MODEL_e89de6cc0364407d9355e8761a3d3300"}},"3e286149a60b4ca09b27c3ffde75307a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e8f8a33d2b54e74ba185f25fa84ce66":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f8f22ba4e224ce1bec351a7072bc6ac":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ab83116fa5548a0b6379282362e0b23","placeholder":"​","style":"IPY_MODEL_54fba074a3dd46be8d2a154eb3cfbdcd","value":" 9.94G/9.94G [01:57&lt;00:00, 103MB/s]"}},"40ceb6510ee8402eaaa88fe0062b80c3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41ab6d5da2ce4365bb720e4d212b30f6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c8cbe59ae47d4227b72a64a998fb71ef","placeholder":"​","style":"IPY_MODEL_da79dac112ce4ee9b1b6c06844c775de","value":"llama-2-7b-chat.Q4_K_M.gguf: 100%"}},"47b60d282dd44924a03abe17dc13eb79":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4c283ddfe62b48b1b3b3e788c7394d0c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4f7287391fbf48cdbd7a34608de3eb79":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"51cbd9afeea4485d8ac39f38abe4e34a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"54fba074a3dd46be8d2a154eb3cfbdcd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"55157c554d3a4f8aa4e9757fb49626cc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"584124e830f54108876bb5ba43e9dda0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"64cd679c34804000b61f7b91a6ef88ee":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fdc6730d5778432bb980016edae4774a","placeholder":"​","style":"IPY_MODEL_66a08e29d4384aab82feaf5e73d7486c","value":"Downloading shards:  50%"}},"66a08e29d4384aab82feaf5e73d7486c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"672da5a5a040476dbb248c68e62860ba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ad5ece063ec643d59221e4f6ee82894f","IPY_MODEL_dbd8f51817634ceaaebe2028f5a48cfb","IPY_MODEL_bc077fe5da564f358bf4d321a3792db5"],"layout":"IPY_MODEL_176e1765d8f6468d9b8d6428aadef114"}},"746cd86ee36c492d908760f254bc8591":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c6659ace946b407a9549a4f604b71f55","placeholder":"​","style":"IPY_MODEL_9f79bc51e11742e28824fe572948c3f5","value":"Downloading (…)model.bin.index.json: 100%"}},"7ae6d85a7fe44bd98978c013094ac232":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b84fc3c71ba480c8dfece282ae50a86":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7bcbed0c9dc04163a3523c62bd6103bf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_34d242f2df4e4f26a3b864fa5f6b9dec","IPY_MODEL_92733967e0284ee7a8db2f1748b9a35f","IPY_MODEL_995df5b2501642b1a3d1399670651c25"],"layout":"IPY_MODEL_4c283ddfe62b48b1b3b3e788c7394d0c"}},"7fa8ceff87bc40e4883073b16bc8a981":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"82c07a4dffec43c78f3a2b9a28be82a0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c19f5497ff24cea97215c090e18b41b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8c70d57d11e241a680ca775f84604ec6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_06148f89071241fd9d8f4bef6e75ff45","IPY_MODEL_1e5475c5ab9e4cc7b300de85b4ef03e7","IPY_MODEL_971a02bd869e43ccb3f843bafbe4f3dd"],"layout":"IPY_MODEL_40ceb6510ee8402eaaa88fe0062b80c3"}},"8cb86dbb99444f17ac0d8d5460bccc6c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8cf2274c55264398b7085b64002809c9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8f58e1eebfe746adad6b63b6fba41775":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_df97d6f6f2b841b4b6a0ed3c35a81c35","placeholder":"​","style":"IPY_MODEL_cab7cd1994d54e3aa817795a4746151e","value":"(…)2882fb562ffccdd1cf0f65402adb/config.json: 100%"}},"9240208765784ec796b30d65ca315027":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"92733967e0284ee7a8db2f1748b9a35f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_a37765cf5bcc489688c9df575873f1fe","max":5064823659,"min":0,"orientation":"horizontal","style":"IPY_MODEL_acea18cf376f4901a7ffd4b4b130d626","value":1646264320}},"93544ea2b6a04f13958e9314cdeea05e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e3e4816950544a8ca295fcfe6d0f4379","IPY_MODEL_9fb27f108ecb4c7b9f787bc542c414b0","IPY_MODEL_3f8f22ba4e224ce1bec351a7072bc6ac"],"layout":"IPY_MODEL_a1f6f53a25904f49bc62cdd9223063b9"}},"971a02bd869e43ccb3f843bafbe4f3dd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_372e18d0f38d455fb091c491ae2528f8","placeholder":"​","style":"IPY_MODEL_ef86517fd1cd427cbe77c38601a674d6","value":" 1/1 [01:09&lt;00:00, 69.71s/it]"}},"995df5b2501642b1a3d1399670651c25":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7b84fc3c71ba480c8dfece282ae50a86","placeholder":"​","style":"IPY_MODEL_c02ddd857e1e438e9466c9f51500419c","value":" 1.65G/5.06G [00:17&lt;00:34, 98.9MB/s]"}},"9dda30b69fb34c9b9a2ce54776bbccf1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9f79bc51e11742e28824fe572948c3f5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9fb27f108ecb4c7b9f787bc542c414b0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3e8f8a33d2b54e74ba185f25fa84ce66","max":9943028044,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8cb86dbb99444f17ac0d8d5460bccc6c","value":9943028044}},"a1f6f53a25904f49bc62cdd9223063b9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a37765cf5bcc489688c9df575873f1fe":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a9d70c251a204380953240c4e2c04081":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f3f9e704932443f19ea0f5ced0c09cc8","placeholder":"​","style":"IPY_MODEL_9dda30b69fb34c9b9a2ce54776bbccf1","value":" 4.08G/4.08G [01:09&lt;00:00, 157MB/s]"}},"acea18cf376f4901a7ffd4b4b130d626":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ad5ece063ec643d59221e4f6ee82894f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_82c07a4dffec43c78f3a2b9a28be82a0","placeholder":"​","style":"IPY_MODEL_ec781af4f26d479ea9e9444ce817f6ab","value":"Fetching 1 files: 100%"}},"b770aa03749046baafb4ea44547192ac":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c15cb7c30b8d4e57a7aeb15c27ddc0e4","max":23950,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8cf2274c55264398b7085b64002809c9","value":23950}},"bc048c2f991b494299a3c911a379765c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bc077fe5da564f358bf4d321a3792db5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2b78a10b1cd641229bc9aa06cf9c6de9","placeholder":"​","style":"IPY_MODEL_c68e4d0d844a4178b5f7114c0a0074c1","value":" 1/1 [00:00&lt;00:00,  1.83it/s]"}},"bfaaa2ed2d0442d7a6e010054fed156f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c02ddd857e1e438e9466c9f51500419c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c0bb975b80b04f628b8af5ff6877b2a1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c15cb7c30b8d4e57a7aeb15c27ddc0e4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6659ace946b407a9549a4f604b71f55":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c68e4d0d844a4178b5f7114c0a0074c1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c8cbe59ae47d4227b72a64a998fb71ef":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9e0a2b2628842d3a9a9f2e5d4b7b735":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cab7cd1994d54e3aa817795a4746151e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cd0314faa9254f529ffc493d8e4bd031":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d16760c760ac47c3b262997a753d571c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"da79dac112ce4ee9b1b6c06844c775de":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dbd8f51817634ceaaebe2028f5a48cfb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4f7287391fbf48cdbd7a34608de3eb79","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9240208765784ec796b30d65ca315027","value":1}},"dd74fcee8211464ea7b5448eb537551c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"def28959529342c4856b3e10edf94304":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7fa8ceff87bc40e4883073b16bc8a981","max":4081004224,"min":0,"orientation":"horizontal","style":"IPY_MODEL_47b60d282dd44924a03abe17dc13eb79","value":4081004224}},"df97d6f6f2b841b4b6a0ed3c35a81c35":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e3e4816950544a8ca295fcfe6d0f4379":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3a41734f698443f99f01c43d1d753254","placeholder":"​","style":"IPY_MODEL_30f2081d8a844e3baf6694b0f433b825","value":"Downloading (…)l-00001-of-00002.bin: 100%"}},"e89de6cc0364407d9355e8761a3d3300":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec781af4f26d479ea9e9444ce817f6ab":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ec80ccc565e24cbc9a12b143738b5457":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eeb3e01b667b46aeae4671434a8aef69":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7ae6d85a7fe44bd98978c013094ac232","placeholder":"​","style":"IPY_MODEL_bc048c2f991b494299a3c911a379765c","value":" 29.0/29.0 [00:00&lt;00:00, 1.30kB/s]"}},"ef86517fd1cd427cbe77c38601a674d6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f3868faf727c4373a4accc9c0448987b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f3f9e704932443f19ea0f5ced0c09cc8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f73cc4ca55d64fdc9377dec685a12da0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fa83598231a1416ca14620fbbc870431":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bfaaa2ed2d0442d7a6e010054fed156f","max":29,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dd74fcee8211464ea7b5448eb537551c","value":29}},"fc2e54eadf754baa91e7ae96339c842f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fdc6730d5778432bb980016edae4774a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
